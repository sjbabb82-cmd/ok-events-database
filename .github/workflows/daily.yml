name: Premium Event Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Requirements
        run: pip install playwright pandas

      - name: Run Engine
        env:
          BROWSER_WS_ENDPOINT: ${{ secrets.BROWSER_WS_ENDPOINT }}
        run: python scraper.py

      - name: Upload Debug HTML
        if: always() # This ensures the file is uploaded even if the script fails
        uses: actions/upload-artifact@v4
        with:
          name: debug-site-html
          path: debug_site.html
          if-no-files-found: ignore # Prevents the job from failing if the file isn't created

      - name: Commit Results
        run: |
          git config --local user.email "bot@github.com"
          git config --local user.name "ProprietaryBot"
          if [ -f master.csv ]; then
            git add master.csv
            git commit -m "Premium Data Update"
            git push
          fi
